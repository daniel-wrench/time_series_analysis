{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master stats doc\n",
    "Here I show the calculation of all the stats for my time series estimation work. These will be put into a function that returns all of them, which we will then calculate the error of for missing data. Note that streamlit code allows calculation for missing data\n",
    "\n",
    "- Compare utils script to reynolds and update accordingly if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spann\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import src.utils as utils\n",
    "import glob\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract variables from CDF files and save as pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subfolders(path):\n",
    "    return sorted(glob.glob(path + \"/*\"))\n",
    "\n",
    "\n",
    "def get_cdf_paths(subfolder):\n",
    "    return sorted(glob.iglob(subfolder + \"/*.cdf\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfi_file_list = get_cdf_paths(\"data/raw/wind/mfi/\")\n",
    "# proton_file_list = get_cdf_paths(\"data/raw/wind/3dp/\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_B = utils.pipeline(\n",
    "#     mfi_file_list[0],\n",
    "#     varlist=[params.timestamp, params_dict[\"Bwind, params_dict[\"Bwind_vec],\n",
    "#     thresholds=params_dict[\"mag_thresh,\n",
    "#     cadence=params_dict[\"dt_hr,\n",
    "# )\n",
    "\n",
    "# data_B = data_B.rename(columns={params_dict[\"Bx: \"Bx\", params_dict[\"By: \"By\", params_dict[\"Bz: \"Bz\"})\n",
    "# data_B = data_B.interpolate(method=\"linear\")\n",
    "# mag_int_hr = data_B[\"2016-01-01 12:00\":]\n",
    "\n",
    "# print(mag_int_hr.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mag_int_hr.to_pickle(\"data/processed/wind/mfi/2016-01-00_12:00.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_protons = utils.pipeline(\n",
    "#     proton_file_list[0],\n",
    "#     varlist=[\n",
    "#         params.timestamp,\n",
    "#         params.np,\n",
    "#         params.nalpha,\n",
    "#         params.Tp,\n",
    "#         params.Talpha,\n",
    "#         params.V_vec,\n",
    "#     ],\n",
    "#     thresholds=params.proton_thresh,\n",
    "#     cadence=params.dt_protons,\n",
    "# )\n",
    "\n",
    "# data_protons = data_protons.rename(\n",
    "#     columns={\n",
    "#         params.Vx: \"Vx\",\n",
    "#         params.Vy: \"Vy\",\n",
    "#         params.Vz: \"Vz\",\n",
    "#         params.np: \"np\",\n",
    "#         params.nalpha: \"nalpha\",\n",
    "#         params.Tp: \"Tp\",\n",
    "#         params.Talpha: \"Talpha\",\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# data_protons = data_protons.interpolate(method=\"linear\")\n",
    "# proton_int = data_protons[\"2016-01-01 12:00\":]\n",
    "\n",
    "# print(proton_int.head())\n",
    "\n",
    "# proton_int.to_pickle(\"data/processed/wind/3dp/2016-01-00_12:00.pkl\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR is currently used for ACF and SF, HR for spectrum and taylor scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure(data, ar_lags, ar_powers):\n",
    "    \"\"\"\n",
    "    Routine to compute the Structure coefficients of a certain series or number of series to different orders\n",
    "    of structure at lags given in ar_lags\n",
    "    Input:\n",
    "            data: pd.DataFrame of data to be analysed. Must have shape (1, N) or (3, N)\n",
    "            ar_lags: The array consisting of lags, being the number of points to shift each of the series\n",
    "            ar_powers: The array consisting of the Structure orders to perform on the series for all lags\n",
    "    Output:\n",
    "            df: The DataFrame containing the structure coefficients corresponding to ar_lags for each order in ar_powers\n",
    "    \"\"\"\n",
    "    # run through ar_lags and ar_powers so that for each power, run through each lag\n",
    "    df = {}\n",
    "\n",
    "    if data.shape[1] == 1:\n",
    "        ax = data.iloc[:, 0].copy()\n",
    "        for i in ar_powers:\n",
    "            array = []\n",
    "            for l in ar_lags:\n",
    "                dax = np.abs(ax.shift(-l) - ax)\n",
    "                strct = dax.pow(i).mean()\n",
    "                array += [strct]\n",
    "            df[str(i)] = array\n",
    "\n",
    "    elif data.shape[1] == 3:\n",
    "        ax = data.iloc[:, 0].copy()\n",
    "        ay = data.iloc[:, 1].copy()\n",
    "        az = data.iloc[:, 2].copy()\n",
    "\n",
    "        for i in ar_powers:\n",
    "            array = []\n",
    "            for l in ar_lags:\n",
    "                dax = np.abs(ax.shift(-l) - ax)\n",
    "                day = np.abs(ay.shift(-l) - ay)\n",
    "                daz = np.abs(az.shift(-l) - az)\n",
    "                strct = (dax.pow(2) + day.pow(2) + daz.pow(2)).pow(0.5).pow(i).mean()\n",
    "                array += [strct]\n",
    "            df[str(i)] = array\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_stats(time_series, var_name, params_dict):\n",
    "    \"\"\"\n",
    "    Calculate a rangle of scale-domain statistics for a given time series\n",
    "    :param time_series: list of pd.Series\n",
    "    :param var_name: str\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    # Compute autocorrelations and power spectra\n",
    "\n",
    "    time_series_low_res = [x.resample(params_dict[\"dt_lr\"]).mean() for x in time_series]\n",
    "\n",
    "    # MATCHES UP WITH LINE 361 IN SRC/PROCESS_DATA.PY\n",
    "    time_lags_lr, acf_lr = utils.compute_nd_acf(\n",
    "        time_series=time_series_low_res, nlags=params_dict[\"nlags_lr\"]\n",
    "    )  # Removing \"S\" from end of dt string\n",
    "\n",
    "    corr_scale_exp_trick = utils.compute_outer_scale_exp_trick(time_lags_lr, acf_lr)\n",
    "    # corr_scale_exp_trick_list.append(corr_scale_exp_trick)\n",
    "\n",
    "    # Use estimate from 1/e method to select fit amount\n",
    "    corr_scale_exp_fit = utils.compute_outer_scale_exp_fit(\n",
    "        time_lags_lr, acf_lr, np.round(2 * corr_scale_exp_trick)\n",
    "    )\n",
    "    # corr_scale_exp_fit_list.append(corr_scale_exp_fit)\n",
    "\n",
    "    corr_scale_int = utils.compute_outer_scale_integral(time_lags_lr, acf_lr)\n",
    "    # corr_scale_int_list.append(corr_scale_int)\n",
    "\n",
    "    time_lags_hr, acf_hr = utils.compute_nd_acf(\n",
    "        time_series=time_series, nlags=params_dict[\"nlags_hr\"]\n",
    "    )\n",
    "\n",
    "    slope_k = np.nan\n",
    "    # ~1min per interval due to spectrum smoothing algorithm\n",
    "    try:\n",
    "        (\n",
    "            slope_i,\n",
    "            slope_k,\n",
    "            break_s,\n",
    "            f_periodogram,\n",
    "            power_periodogram,\n",
    "            p_smooth,\n",
    "            xi,\n",
    "            xk,\n",
    "            pi,\n",
    "            pk,\n",
    "        ) = utils.compute_spectral_stats(\n",
    "            time_series=time_series,\n",
    "            f_min_inertial=params_dict[\"f_min_inertial\"],\n",
    "            f_max_inertial=params_dict[\"f_max_inertial\"],\n",
    "            f_min_kinetic=params_dict[\"f_min_kinetic\"],\n",
    "            f_max_kinetic=params_dict[\"f_max_kinetic\"],\n",
    "        )\n",
    "\n",
    "        # inertial_slope_list.append(slope_i)\n",
    "        # kinetic_slope_list.append(slope_k)\n",
    "        # spectral_break_list.append(break_s)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error: spectral stats calculation failed: {}\".format(e))\n",
    "        # print(\"Interval timestamp: {}\".format(int_start))\n",
    "        # inertial_slope_list.append(np.nan)\n",
    "        # kinetic_slope_list.append(np.nan)\n",
    "        # spectral_break_list.append(np.nan)\n",
    "        # slope_k = None\n",
    "\n",
    "    if params_dict[\"tau_min\"] is not None:\n",
    "        taylor_scale_u, taylor_scale_u_std = utils.compute_taylor_chuychai(\n",
    "            time_lags_hr,\n",
    "            acf_hr,\n",
    "            tau_min=params_dict[\"tau_min\"],\n",
    "            tau_max=params_dict[\"tau_max\"],\n",
    "        )\n",
    "\n",
    "        if not np.isnan(slope_k):\n",
    "            taylor_scale_c, taylor_scale_c_std = utils.compute_taylor_chuychai(\n",
    "                time_lags_hr,\n",
    "                acf_hr,\n",
    "                tau_min=params_dict[\"tau_min\"],\n",
    "                tau_max=params_dict[\"tau_max\"],\n",
    "                q=slope_k,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            taylor_scale_c = np.nan\n",
    "            taylor_scale_c_std = np.nan\n",
    "    else:\n",
    "        taylor_scale_u = np.nan\n",
    "        taylor_scale_u_std = np.nan\n",
    "        taylor_scale_c = np.nan\n",
    "        taylor_scale_c_std = np.nan\n",
    "\n",
    "    int_lr_df = pd.concat(time_series_low_res, axis=1)\n",
    "    sfns = structure(int_lr_df, np.arange(1, round(0.2 * len(int_lr_df))), [1, 2, 3, 4])\n",
    "\n",
    "    # Calculate kurtosis (currently not component-wise)\n",
    "    sdk = sfns[[\"2\", \"4\"]]\n",
    "    sdk.columns = [\"2\", \"4\"]\n",
    "    sdk[\"2^2\"] = sdk[\"2\"] ** 2\n",
    "    kurtosis = sdk[\"4\"].div(sdk[\"2^2\"])\n",
    "\n",
    "    # Store these results in a dictionary\n",
    "    stats_dict = {\n",
    "        var_name: {\n",
    "            \"time_series\": time_series_low_res,\n",
    "            \"times\": time_lags_lr,\n",
    "            \"time_lags_hr\": time_lags_hr,\n",
    "            \"xi\": xi,\n",
    "            \"xk\": xk,\n",
    "            \"pi\": pi,\n",
    "            \"pk\": pk,\n",
    "            \"cr\": acf_lr,\n",
    "            \"acf_hr\": acf_hr,\n",
    "            \"qi\": slope_i,\n",
    "            \"qk\": slope_k,\n",
    "            \"break_s\": break_s,\n",
    "            \"corr_scale_exp_trick\": corr_scale_exp_trick,\n",
    "            \"corr_scale_exp_fit\": corr_scale_exp_fit,\n",
    "            \"corr_scale_int\": corr_scale_int,\n",
    "            \"taylor_scale_u\": taylor_scale_u,\n",
    "            \"taylor_scale_u_std\": taylor_scale_u_std,\n",
    "            \"taylor_scale_c\": taylor_scale_c,\n",
    "            \"taylor_scale_c_std\": taylor_scale_c_std,\n",
    "            \"f_periodogram\": f_periodogram,\n",
    "            \"power_periodogram\": power_periodogram,\n",
    "            \"p_smooth\": p_smooth,\n",
    "            \"sfn\": sfns,  # multiple orders\n",
    "            \"sdk\": kurtosis,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return int_lr_df, stats_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(stats_dict):\n",
    "    # Plot the results\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(15, 6))\n",
    "    # Adjust the vertical spacing\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "    # Time Series in the first panel\n",
    "    axs[0, 0].plot(\n",
    "        np.arange(len(stats_dict[\"time_series\"])),\n",
    "        stats_dict[\"time_series\"],\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    axs[0, 0].set_title(\"Time series $x(t)$\")\n",
    "\n",
    "    # ACF LR in the second panel\n",
    "    axs[0, 1].plot(stats_dict[\"times\"], stats_dict[\"cr\"], color=\"green\")\n",
    "    axs[0, 1].axvline(stats_dict[\"corr_scale_exp_trick\"], color=\"black\", ls=\"--\")\n",
    "    axs[0, 1].axhline(np.exp(-1), color=\"black\", ls=\"--\")\n",
    "    axs[0, 1].set_title(\"Autocorrelation $R(\\\\tau)$ (low-res)\")\n",
    "\n",
    "    # ACF HR in the third panel\n",
    "    axs[0, 2].plot(stats_dict[\"time_lags_hr\"], stats_dict[\"acf_hr\"], color=\"red\")\n",
    "    axs[0, 2].set_title(\"Autocorrelation $R(\\\\tau)$ (high-res)\")\n",
    "\n",
    "    # Periodogram in the fourth panel\n",
    "    axs[1, 0].plot(\n",
    "        stats_dict[\"f_periodogram\"],\n",
    "        stats_dict[\"power_periodogram\"],\n",
    "        c=\"purple\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    axs[1, 0].plot(stats_dict[\"f_periodogram\"], stats_dict[\"p_smooth\"], c=\"purple\")\n",
    "    axs[1, 0].plot(stats_dict[\"xi\"], stats_dict[\"pi\"] * 3, c=\"black\", ls=\"--\")\n",
    "    axs[1, 0].plot(stats_dict[\"xk\"], stats_dict[\"pk\"] * 3, c=\"black\", ls=\"--\")\n",
    "    axs[1, 0].axvline(stats_dict[\"break_s\"], color=\"black\", ls=\"dotted\")\n",
    "    axs[1, 0].set_title(\"Periodogram $P(f)$\")\n",
    "    axs[1, 0].set_xscale(\"log\")\n",
    "    axs[1, 0].set_yscale(\"log\")\n",
    "\n",
    "    # SFN in the fifth panel\n",
    "    axs[1, 1].plot(stats_dict[\"sfn\"], color=\"orange\")\n",
    "    axs[1, 1].set_title(\"Structure function $S_2(\\\\tau)$\")\n",
    "    axs[1, 1].set_xscale(\"log\")\n",
    "    axs[1, 1].set_yscale(\"log\")\n",
    "\n",
    "    # Kurtosis in the sixth panel\n",
    "    axs[1, 2].plot(stats_dict[\"sdk\"], color=\"cyan\")\n",
    "    axs[1, 2].set_title(\"Kurtosis $K(\\\\tau)=S_4/S_2^2$\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_int_hr = pd.read_pickle(\"data/processed/wind/mfi/2016-01-00_12:00.pkl\")\n",
    "\n",
    "# Frequency bounds are taken from Wang et al. (2018, JGR)\n",
    "mag_params = {\n",
    "    \"f_min_inertial\": 0.005,\n",
    "    \"f_max_inertial\": 0.2,\n",
    "    \"f_min_kinetic\": 0.5,\n",
    "    \"f_max_kinetic\": 1.4,\n",
    "    \"nlags_lr\": 2000,\n",
    "    \"nlags_hr\": 100,\n",
    "    \"dt_lr\": \"5S\",\n",
    "    \"tau_min\": 10,\n",
    "    \"tau_max\": 50,\n",
    "}\n",
    "\n",
    "proton_int = pd.read_pickle(\"data/processed/wind/3dp/2016-01-00_12:00.pkl\")\n",
    "proton_params = {\n",
    "    \"f_min_inertial\": None,\n",
    "    \"f_max_inertial\": None,\n",
    "    \"f_min_kinetic\": None,\n",
    "    \"f_max_kinetic\": None,\n",
    "    \"nlags_lr\": 2000,\n",
    "    \"nlags_hr\": 100,\n",
    "    \"dt_lr\": \"5S\",\n",
    "    \"tau_min\": None,\n",
    "    \"tau_max\": None,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spann\\AppData\\Local\\Temp\\ipykernel_12956\\946917652.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sdk[\"2^2\"] = sdk[\"2\"] ** 2\n"
     ]
    }
   ],
   "source": [
    "# Get raw time series and turbulent quantities\n",
    "flr, flt = calculate_all_stats([mag_int_hr.Bx], \"Bx\", mag_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary for later plotting\n",
    "flr.to_pickle(\"data/processed/\" + \"Bx_raw.pkl\")\n",
    "with open(\"data/processed/\" + \"Bx_turb.pkl\", \"wb\") as file:\n",
    "    pickle.dump(flt, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spann\\AppData\\Local\\Temp\\ipykernel_12956\\946917652.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sdk[\"2^2\"] = sdk[\"2\"] ** 2\n"
     ]
    }
   ],
   "source": [
    "# Get raw time series and turbulent quantities\n",
    "flr, flt = calculate_all_stats(\n",
    "    [mag_int_hr.Bx, mag_int_hr.By, mag_int_hr.Bz], \"B\", mag_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "flr.to_pickle(\"data/processed/\" + \"B_raw.pkl\")\n",
    "with open(\"data/processed/\" + \"B_turb.pkl\", \"wb\") as file:\n",
    "    pickle.dump(flt, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spann\\AppData\\Local\\Temp\\ipykernel_12956\\946917652.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sdk[\"2^2\"] = sdk[\"2\"] ** 2\n"
     ]
    }
   ],
   "source": [
    "# Get raw time series and turbulent quantities\n",
    "flr, flt = calculate_all_stats(\n",
    "    [proton_int.Vx, proton_int.Vy, proton_int.Vz], \"V\", proton_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "\n",
    "flr.to_pickle(\"data/processed/\" + \"proton_raw.pkl\")\n",
    "with open(\"data/processed/\" + \"proton_turb.pkl\", \"wb\") as file:\n",
    "    pickle.dump(flt, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

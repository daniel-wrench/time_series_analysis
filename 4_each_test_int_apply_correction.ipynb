{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. FOR EACH INTERVAL IN TEST SET: apply correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import src.sf_funcs as sf\n",
    "import glob\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 10\n",
    "times_to_gap = 3\n",
    "pwrl_range = [10,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing lookup table\n",
    "lookup_table_2d = pd.read_csv(f\"data/processed/lookup_table_2d_{n_bins}bins.csv\", index_col=0)\n",
    "lookup_table_3d = pd.read_csv(f\"data/processed/lookup_table_3d_{n_bins}bins.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just importing one file at a time!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing processed time series and structure functions\n",
    "input_file_list = [sorted(glob.glob(\"data/processed/wind/wi_*05.pkl\"))][0]\n",
    "#input_file_list = [sorted(glob.glob(\"data/processed/psp/test/psp_*v02.pkl\"))][0]\n",
    "\n",
    "file_index_test = 1 # this simply refers to one of the files in the test files, not the \"file_index\" variable referring to the original raw file \n",
    "\n",
    "# Just importing one file at a time!\n",
    "files_metadata, ints_metadata, ints, ints_gapped_metadata, ints_gapped, sfs, sfs_gapped = sf.load_and_concatenate_dataframes(\n",
    "    [input_file_list[file_index_test]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['naive', 'lint', 'corrected_2d', 'corrected_3d'], dtype=object)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfs_gapped_corrected.gap_handling.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting interpolated test set intervals using 2D error heatmap with 10 bins\n"
     ]
    }
   ],
   "source": [
    "# Apply 2D and 3D scaling to test set, report avg errors\n",
    "print(f\"Correcting interpolated test set intervals using 2D error heatmap with {n_bins} bins\")\n",
    "sfs_lint_corrected_2d = sf.compute_scaling(\n",
    "    sfs_gapped[sfs_gapped[\"gap_handling\"]==\"lint\"], \"missing_percent\", lookup_table_2d\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting interpolated test set intervals using 3D error heatmap with 10 bins\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correcting interpolated test set intervals using 3D error heatmap with {n_bins} bins\")\n",
    "sfs_lint_corrected_2d_3d = sf.compute_scaling_3d(\n",
    "    sfs_lint_corrected_2d[sfs_lint_corrected_2d[\"gap_handling\"]==\"lint\"], \"missing_percent\", lookup_table_3d\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_wide =  sfs_lint_corrected_2d_3d[[\"file_index\", \"int_index\", \"version\", \"lag\", \"missing_percent\", \"sf_2_corrected_2d\", \"sf_2_corrected_3d\"]]\n",
    "correction_long = pd.wide_to_long(correction_wide, [\"sf_2\"], i=[\"file_index\", \"int_index\", \"version\", \"lag\", \"missing_percent\"], j=\"gap_handling\", sep=\"_\", suffix=r\"\\w+\")\n",
    "correction_bounds_wide =  sfs_lint_corrected_2d_3d[[\"file_index\", \"int_index\", \"version\", \"lag\", \"missing_percent\", \"sf_2_lower_corrected_2d\", \"sf_2_lower_corrected_3d\", \"sf_2_upper_corrected_2d\", \"sf_2_upper_corrected_3d\"]]\n",
    "correction_bounds_long = pd.wide_to_long(correction_bounds_wide, [\"sf_2_lower\", \"sf_2_upper\"], i=[\"file_index\", \"int_index\", \"version\", \"lag\", \"missing_percent\"], j=\"gap_handling\", sep=\"_\", suffix=r\"\\w+\")\n",
    "\n",
    "corrections_long = pd.merge(correction_long, correction_bounds_long, how=\"inner\", on=[\"file_index\", \"int_index\", \"version\", \"lag\", \"missing_percent\", \"gap_handling\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the corrections, now as a form of \"gap_handling\", back to the gapped SF dataframe\n",
    "sfs_gapped_corrected = pd.concat([sfs_gapped, corrections_long])\n",
    "\n",
    "# Merging the original SFs with the corrected ones to then calculate errors\n",
    "sfs_gapped_corrected = pd.merge(sfs, sfs_gapped_corrected, how=\"inner\", on=[\"file_index\", \"int_index\", \"lag\"], suffixes=(\"_orig\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_index</th>\n",
       "      <th>file_index</th>\n",
       "      <th>lag</th>\n",
       "      <th>n_orig</th>\n",
       "      <th>missing_percent_orig</th>\n",
       "      <th>sf_2_orig</th>\n",
       "      <th>sf_2_se_orig</th>\n",
       "      <th>n</th>\n",
       "      <th>missing_percent</th>\n",
       "      <th>sf_2</th>\n",
       "      <th>sf_2_se</th>\n",
       "      <th>version</th>\n",
       "      <th>gap_handling</th>\n",
       "      <th>sf_2_lower</th>\n",
       "      <th>sf_2_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019208</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>4672.0</td>\n",
       "      <td>53.275328</td>\n",
       "      <td>0.019315</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0</td>\n",
       "      <td>naive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019208</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>4672.0</td>\n",
       "      <td>53.275328</td>\n",
       "      <td>0.010518</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0</td>\n",
       "      <td>lint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019208</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>57.235724</td>\n",
       "      <td>0.020219</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>1</td>\n",
       "      <td>naive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019208</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>57.235724</td>\n",
       "      <td>0.008825</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>1</td>\n",
       "      <td>lint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019208</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>8238.0</td>\n",
       "      <td>17.611761</td>\n",
       "      <td>0.019699</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>2</td>\n",
       "      <td>naive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11983</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>999.0</td>\n",
       "      <td>9001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.955424</td>\n",
       "      <td>0.029148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.326630</td>\n",
       "      <td>2.575671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>corrected_3d</td>\n",
       "      <td>1.653954</td>\n",
       "      <td>5.817839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11984</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>999.0</td>\n",
       "      <td>9001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.955424</td>\n",
       "      <td>0.029148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.446617</td>\n",
       "      <td>1.050516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>corrected_2d</td>\n",
       "      <td>0.758517</td>\n",
       "      <td>11.022705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11985</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>999.0</td>\n",
       "      <td>9001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.955424</td>\n",
       "      <td>0.029148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.446617</td>\n",
       "      <td>1.964575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>corrected_3d</td>\n",
       "      <td>1.849105</td>\n",
       "      <td>2.095428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11986</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>999.0</td>\n",
       "      <td>9001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.955424</td>\n",
       "      <td>0.029148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.729808</td>\n",
       "      <td>2.154849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>corrected_2d</td>\n",
       "      <td>1.952143</td>\n",
       "      <td>2.323564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11987</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>999.0</td>\n",
       "      <td>9001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.955424</td>\n",
       "      <td>0.029148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.729808</td>\n",
       "      <td>2.134483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>corrected_3d</td>\n",
       "      <td>1.486083</td>\n",
       "      <td>3.786662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11988 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       int_index  file_index    lag  n_orig  missing_percent_orig  sf_2_orig  \\\n",
       "0              0           4    1.0    9999                   0.0   0.019208   \n",
       "1              0           4    1.0    9999                   0.0   0.019208   \n",
       "2              0           4    1.0    9999                   0.0   0.019208   \n",
       "3              0           4    1.0    9999                   0.0   0.019208   \n",
       "4              0           4    1.0    9999                   0.0   0.019208   \n",
       "...          ...         ...    ...     ...                   ...        ...   \n",
       "11983          0           4  999.0    9001                   0.0   1.955424   \n",
       "11984          0           4  999.0    9001                   0.0   1.955424   \n",
       "11985          0           4  999.0    9001                   0.0   1.955424   \n",
       "11986          0           4  999.0    9001                   0.0   1.955424   \n",
       "11987          0           4  999.0    9001                   0.0   1.955424   \n",
       "\n",
       "       sf_2_se_orig       n  missing_percent      sf_2   sf_2_se  version  \\\n",
       "0          0.001052  4672.0        53.275328  0.019315  0.001630        0   \n",
       "1          0.001052  4672.0        53.275328  0.010518  0.001630        0   \n",
       "2          0.001052  4276.0        57.235724  0.020219  0.001660        1   \n",
       "3          0.001052  4276.0        57.235724  0.008825  0.001660        1   \n",
       "4          0.001052  8238.0        17.611761  0.019699  0.001224        2   \n",
       "...             ...     ...              ...       ...       ...      ...   \n",
       "11983      0.029148     NaN        60.326630  2.575671       NaN        0   \n",
       "11984      0.029148     NaN        80.446617  1.050516       NaN        1   \n",
       "11985      0.029148     NaN        80.446617  1.964575       NaN        1   \n",
       "11986      0.029148     NaN        31.729808  2.154849       NaN        2   \n",
       "11987      0.029148     NaN        31.729808  2.134483       NaN        2   \n",
       "\n",
       "       gap_handling  sf_2_lower  sf_2_upper  \n",
       "0             naive         NaN         NaN  \n",
       "1              lint         NaN         NaN  \n",
       "2             naive         NaN         NaN  \n",
       "3              lint         NaN         NaN  \n",
       "4             naive         NaN         NaN  \n",
       "...             ...         ...         ...  \n",
       "11983  corrected_3d    1.653954    5.817839  \n",
       "11984  corrected_2d    0.758517   11.022705  \n",
       "11985  corrected_3d    1.849105    2.095428  \n",
       "11986  corrected_2d    1.952143    2.323564  \n",
       "11987  corrected_3d    1.486083    3.786662  \n",
       "\n",
       "[11988 rows x 15 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfs_gapped_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate lag-scale errors (sf_2_pe)\n",
    "This is the first time we calculate these errors, for this specific dataset (they were calculated before for the training set)\n",
    "\n",
    "Previously this didn't work as we had two sf_2_orig columns as the result of merging a dataframe that had already previously been merged. However, this initial merge is no longer taking place, as it is only now that we are calculating any errors *of any sort, including lag-specific ones*, for this particular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_gapped_corrected[\"sf_2_pe\"] = (sfs_gapped_corrected[\"sf_2\"] - sfs_gapped_corrected[\"sf_2_orig\"]) / sfs_gapped_corrected[\"sf_2_orig\"] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate interval-scale errors\n",
    "This is the first time we do this. We do not need these values for the training set, because we only use that for calculating the correction factor, which uses lag-scale errors.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding rows as placeholders for when we correct with 2D and 3D heatmaps and want to calculate errors\n",
    "\n",
    "dup_df = ints_gapped_metadata.replace([\"naive\",\"lint\"], [\"corrected_2d\", \"corrected_3d\"])\n",
    "ints_gapped_metadata = pd.concat([ints_gapped_metadata, dup_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in files_metadata.file_index.unique():\n",
    "    for j in range(len(ints_metadata[\"file_index\"]==i)):\n",
    "        for k in range(times_to_gap):\n",
    "            for gap_handling in sfs_gapped_corrected.gap_handling.unique():\n",
    "            # Calculate MAPE for 2D and 3D corrected SFs\n",
    "                \n",
    "                ints_gapped_metadata.loc[\n",
    "                    (ints_gapped_metadata[\"file_index\"]==i) &\n",
    "                    (ints_gapped_metadata[\"int_index\"]==j) & \n",
    "                    (ints_gapped_metadata[\"version\"]==k) & \n",
    "                    (ints_gapped_metadata[\"gap_handling\"]==gap_handling), \n",
    "                    \"mape\"] = np.mean(\n",
    "                    np.abs(\n",
    "                        sfs_gapped_corrected.loc[\n",
    "                            (sfs_gapped_corrected[\"file_index\"]==i) &\n",
    "                            (sfs_gapped_corrected[\"int_index\"]==j) & \n",
    "                            (sfs_gapped_corrected[\"version\"]==k) & \n",
    "                            (sfs_gapped_corrected[\"gap_handling\"]==gap_handling), \n",
    "                            \"sf_2_pe\"]))\n",
    "\n",
    "                ints_gapped_metadata.loc[\n",
    "                    (ints_gapped_metadata[\"file_index\"]==i) &\n",
    "                    (ints_gapped_metadata[\"int_index\"]==j) & \n",
    "                    (ints_gapped_metadata[\"version\"]==k) & \n",
    "                    (ints_gapped_metadata[\"gap_handling\"]==gap_handling), \n",
    "                    \"mpe\"] = np.mean(\n",
    "                        sfs_gapped_corrected.loc[\n",
    "                            (sfs_gapped_corrected[\"file_index\"]==i) &\n",
    "                            (sfs_gapped_corrected[\"int_index\"]==j) & \n",
    "                            (sfs_gapped_corrected[\"version\"]==k) & \n",
    "                            (sfs_gapped_corrected[\"gap_handling\"]==gap_handling), \n",
    "                            \"sf_2_pe\"])\n",
    "                \n",
    "                # Calculate power-law slope for 2D and 3D corrected SFs\n",
    "                current_int = sfs_gapped_corrected.loc[\n",
    "                    (sfs_gapped_corrected[\"file_index\"]==i) &\n",
    "                    (sfs_gapped_corrected[\"int_index\"]==j) & \n",
    "                    (sfs_gapped_corrected[\"version\"]==k) & \n",
    "                    (sfs_gapped_corrected[\"gap_handling\"]==gap_handling)]\n",
    "\n",
    "                # Fit a line to the log-log plot of the structure function over the given range\n",
    "\n",
    "                slope = np.polyfit(\n",
    "                    np.log(current_int.loc[(current_int[\"lag\"] >= pwrl_range[0]) & (current_int[\"lag\"] <= pwrl_range[1]), \"lag\"]),\n",
    "                    np.log(current_int.loc[(current_int[\"lag\"] >= pwrl_range[0]) & (current_int[\"lag\"] <= pwrl_range[1]), \"sf_2\"]),\n",
    "                    1,\n",
    "                )[0]\n",
    "\n",
    "                ints_gapped_metadata.loc[\n",
    "                    (ints_gapped_metadata[\"file_index\"]==i) &\n",
    "                    (ints_gapped_metadata[\"int_index\"]==j) & \n",
    "                    (ints_gapped_metadata[\"version\"]==k) & \n",
    "                    (ints_gapped_metadata[\"gap_handling\"]==gap_handling), \n",
    "                    \"slope\"] = slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = np.polyfit(\n",
    "    np.log(current_int.loc[(current_int[\"lag\"] >= pwrl_range[0]) & (current_int[\"lag\"] <= pwrl_range[1]), \"lag\"]),\n",
    "    np.log(current_int.loc[(current_int[\"lag\"] >= pwrl_range[0]) & (current_int[\"lag\"] <= pwrl_range[1]), \"sf_2\"]),\n",
    "    1,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate slope errors\n",
    "ints_gapped_metadata = pd.merge(ints_gapped_metadata, ints_metadata.drop([\"int_start\", \"int_end\"], axis=1), how=\"inner\", on=[\"file_index\", \"int_index\"], suffixes=(\"\", \"_orig\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe come back to this method of getting true slopes, could be fun\n",
    "\n",
    "# # Create a dictionary from df2 with composite keys\n",
    "# value2_dict = df2.set_index(['key1', 'key2'])['value2'].to_dict()\n",
    "\n",
    "# # Create a composite key in df1 and map the values\n",
    "# df1['composite_key'] = list(zip(df1['key1'], df1['key2']))\n",
    "# df1['value2'] = df1['composite_key'].map(value2_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints_gapped_metadata[\"slope_pe\"] = (ints_gapped_metadata[\"slope\"] - ints_gapped_metadata[\"slope_orig\"]) / ints_gapped_metadata[\"slope_orig\"] * 100\n",
    "ints_gapped_metadata[\"slope_ape\"] = np.abs(ints_gapped_metadata[\"slope_pe\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataframes in one big pickle file\n",
    "output_file_path = input_file_list[file_index_test].replace(\".pkl\", \"_corrected.pkl\")\n",
    "\n",
    "with open(output_file_path, \"wb\") as f:\n",
    "    pickle.dump(\n",
    "        {\n",
    "            \"files_metadata\": files_metadata,\n",
    "            \"ints_metadata\": ints_metadata,\n",
    "            \"ints\": ints,\n",
    "            \"ints_gapped_metadata\": ints_gapped_metadata,\n",
    "            \"ints_gapped\": ints_gapped,\n",
    "            \"sfs\": sfs,\n",
    "            \"sfs_gapped\": sfs_gapped_corrected,\n",
    "        },\n",
    "        f,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['naive', 'lint', 'corrected_2d', 'corrected_3d'], dtype=object)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfs_gapped_corrected.gap_handling.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
